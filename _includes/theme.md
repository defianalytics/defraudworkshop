
The main goal of the ODD workshop is to bring together academics, industry and government researchers, and practitioners to discuss and reflect on outlier mining challenges. Outlier detection methods are often applied to numerous real-world applications like security, healthcare, finance. These tasks affect humans in some way, and hence, ensuring the fairness of such methods is paramount. Fairness relates to developing unbiased decision policies whose outcomes are not dependent on any sensitive features or variables such as gender and race. Transparency is another factor, linked to the fairness of methods, where the decision made by the designed methods should be understandable in order to ensure that the methods are not biased towards specific groups.

We want to highlight issues related to fairness and transparency and aim to increase awareness of the following topics:

- How can we measure the bias in the outlier detection methods?
- How can we ensure that outlier identification does not produce unjust outcomes for protected minority groups (such as age/race/sex)?
- How can we prevent a negative feedback loop, of unfair outlier detection based on historically biased data (such as over policing in minority neighborhoods)?
- How can we make outlier detection methods transparent?
- How can we employ deep learning models for detecting and ensuring fairness in outlier detection systems?
- How can we ensure statistical parity in our analysis, such that the outlier detection outcomes are independent of class memberships?
- How can we employ adversarial learning mechanisms for outlier detection to ensure fairness?
